{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHxyiE0JTZxp"
      },
      "source": [
        "# Assignment 2\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cmTXalrJTZxx"
      },
      "source": [
        "## 1 Data\n",
        "\n",
        "We will conduct experiments on Conll2003, which contains 14041 sentences, and each sentence is an\u0002notated with the corresponding named entity tags. You can download the dataset from the following\n",
        "link: https://data.deepai.org/conll2003.zip. We only focus on the token and NER tags, which are\n",
        "the first and last columns in the dataset. The dataset is in the IOB format, which is a common format\n",
        "for named entity recognition. The IOB format 1 is a simple way to represent the named entity tags. For\n",
        "example, the sentence “I went to New York City last week” is annotated as follows:\n",
        "\n",
        "    I O\n",
        "    went O\n",
        "    to O\n",
        "    New B-LOC\n",
        "    York I-LOC\n",
        "    City I-LOC\n",
        "    last O\n",
        "    week O"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "PsUJfgqITZx0"
      },
      "outputs": [],
      "source": [
        "def load_data():\n",
        "    # load data\n",
        "    import os\n",
        "    import numpy as np\n",
        "\n",
        "    data_dir = os.path.join(os.getcwd(), \"data\")\n",
        "    train_path = os.path.join(data_dir, 'train.txt')\n",
        "    valid_path = os.path.join(data_dir, 'valid.txt')\n",
        "    test_path  = os.path.join(data_dir, 'test.txt')\n",
        "\n",
        "    with open(train_path, 'r', encoding=\"utf-8\") as f:\n",
        "        train_raw = [l.strip() for l in f.readlines()]\n",
        "        train = list()\n",
        "        start_of_sentence = 0\n",
        "        for i in range(len(train_raw)):\n",
        "            if train_raw[i] == '': # end of sentence\n",
        "                sent = list()\n",
        "                tags = list()\n",
        "                for l in train_raw[start_of_sentence:i]:\n",
        "                    l = l.split(' ')\n",
        "                    sent.append(l[0])\n",
        "                    tags.append(l[-1])\n",
        "                train.append((sent, tags))\n",
        "                start_of_sentence = i+1\n",
        "\n",
        "    with open(valid_path, 'r', encoding=\"utf-8\") as f:\n",
        "        valid_raw = [l.strip() for l in f.readlines()]\n",
        "        valid = list()\n",
        "        start_of_sentence = 0\n",
        "        for i in range(len(valid_raw)):\n",
        "            if valid_raw[i] == '': # end of sentence\n",
        "                sent = list()\n",
        "                tags = list()\n",
        "                for l in valid_raw[start_of_sentence:i]:\n",
        "                    l = l.split(' ')\n",
        "                    sent.append(l[0])\n",
        "                    tags.append(l[-1])\n",
        "                valid.append((sent, tags))\n",
        "                start_of_sentence = i+1\n",
        "    with open(test_path, 'r', encoding=\"utf-8\") as f:\n",
        "        test_raw = [l.strip() for l in f.readlines()]\n",
        "        test = list()\n",
        "        start_of_sentence = 0\n",
        "        for i in range(len(test_raw)):\n",
        "            if test_raw[i] == '': # end of sentence\n",
        "                sent = list()\n",
        "                tags = list()\n",
        "                for l in test_raw[start_of_sentence:i]:\n",
        "                    l = l.split(' ')\n",
        "                    sent.append(l[0])\n",
        "                    tags.append(l[-1])\n",
        "                test.append((sent, tags))\n",
        "                start_of_sentence = i+1\n",
        "    \n",
        "    return train, valid, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "y-l530wXTZx3"
      },
      "outputs": [],
      "source": [
        "train, valid, test = load_data()\n",
        "# print(train)\n",
        "# print(valid)\n",
        "# print(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohtvrOI6TZx4"
      },
      "source": [
        "## 2 Tagger\n",
        "    You will train your tagger on the train set and evaluate it on the dev set. And then, you may tune the\n",
        "    hyperparameters of your tagger to get the best performance on the dev set. Finally, you will evaluate\n",
        "    your tagger on the test set to get the final performance.\n",
        "\n",
        "    https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging)\n",
        "\n",
        "    There are some key points you should pay attention to:\n",
        "    • You will batch the sentences in the dataset to accelerate the training process. To batch the sentences, you may need to pad the sentences to the same length.\n",
        "    • You are free to design the model architecture with (Bi)LSTM or Transformer unit for each part, but please do not use any pretrained weights in your basic taggers.\n",
        "    • You will adjust the hyperparameters of your tagger to get the best performance on the dev set. The hyperparameters include the learning rate, batch size, the number of hidden units, the number of\n",
        "    layers, the dropout rate, etc.\n",
        "    • You will use seqeval to evaluate your tagger on the dev set and the test set.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoXmuUzvTZx5"
      },
      "source": [
        "### 2.1 LSTM Tagger\n",
        "    We will first use an LSTM tagger to solve the NER problem. There is a very simple implementation of the\n",
        "    LSTM tagger on PyTorch website https://pytorch.org/tutorials/beginner/nlp/sequence_models_\n",
        "    tutorial.html. You can refer to this implementation to implement your LSTM tagger.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG7tAkhvTZx7",
        "outputId": "f7920777-7819-4479-c805-f72524fe268a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f1605488d70>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Author: Robert Guthrie\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qy_fllCCTZx8",
        "outputId": "1e5530dc-16ae-454a-884a-0f70706bf538"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0187,  0.1713, -0.2944]],\n",
            "\n",
            "        [[-0.3521,  0.1026, -0.2971]],\n",
            "\n",
            "        [[-0.3191,  0.0781, -0.1957]],\n",
            "\n",
            "        [[-0.1634,  0.0941, -0.1637]],\n",
            "\n",
            "        [[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>)\n",
            "(tensor([[[-0.3368,  0.0959, -0.0538]]], grad_fn=<StackBackward0>), tensor([[[-0.9825,  0.4715, -0.0633]]], grad_fn=<StackBackward0>))\n"
          ]
        }
      ],
      "source": [
        "lstm = nn.LSTM(3, 3)  # Input dim is 3, output dim is 3\n",
        "inputs = [torch.randn(1, 3) for _ in range(5)]  # make a sequence of length 5\n",
        "\n",
        "# initialize the hidden state.\n",
        "hidden = (torch.randn(1, 1, 3),\n",
        "          torch.randn(1, 1, 3))\n",
        "for i in inputs:\n",
        "    # Step through the sequence one element at a time.\n",
        "    # after each step, hidden contains the hidden state.\n",
        "    out, hidden = lstm(i.view(1, 1, -1), hidden)\n",
        "\n",
        "# alternatively, we can do the entire sequence all at once.\n",
        "# the first value returned by LSTM is all of the hidden states throughout\n",
        "# the sequence. the second is just the most recent hidden state\n",
        "# (compare the last slice of \"out\" with \"hidden\" below, they are the same)\n",
        "# The reason for this is that:\n",
        "# \"out\" will give you access to all hidden states in the sequence\n",
        "# \"hidden\" will allow you to continue the sequence and backpropagate,\n",
        "# by passing it as an argument  to the lstm at a later time\n",
        "# Add the extra 2nd dimension\n",
        "inputs = torch.cat(inputs).view(len(inputs), 1, -1)\n",
        "hidden = (torch.randn(1, 1, 3), torch.randn(1, 1, 3))  # clean out hidden state\n",
        "out, hidden = lstm(inputs, hidden)\n",
        "print(out)\n",
        "print(hidden)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-y-fzjY6TZx9",
        "outputId": "47127353-38e5-4974-9043-ced9547e4000"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'O': 0, 'B-ORG': 1, 'B-MISC': 2, 'B-PER': 3, 'I-PER': 4, 'B-LOC': 5, 'I-ORG': 6, 'I-MISC': 7, 'I-LOC': 8}\n"
          ]
        }
      ],
      "source": [
        "# prepare the data\n",
        "def prepare_sequence(seq, to_ix):\n",
        "    idxs = [to_ix[w] for w in seq]\n",
        "    return torch.tensor(idxs, dtype=torch.long)\n",
        "\n",
        "# Tags are: DET - determiner; NN - noun; V - verb\n",
        "# For example, the word \"The\" is a determiner\n",
        "training_data = train\n",
        "word_to_ix = {}\n",
        "tag_to_ix = {}  # Assign each tag with a unique index\n",
        "# For each words-list (sentence) and tags-list in each tuple of training_data\n",
        "for sent, tags in training_data:\n",
        "    for word in sent:\n",
        "        if word not in word_to_ix:  # word has not been assigned an index yet\n",
        "            word_to_ix[word] = len(word_to_ix)  # Assign each word with a unique index\n",
        "    for word in tags:\n",
        "        if word not in tag_to_ix:  # word has not been assigned an index yet\n",
        "            tag_to_ix[word] = len(tag_to_ix)  # Assign each word with a unique index\n",
        "\n",
        "# print(word_to_ix)\n",
        "print(tag_to_ix)\n",
        "\n",
        "# These will usually be more like 32 or 64 dimensional.\n",
        "# We will keep them small, so we can see how the weights change as we train.\n",
        "EMBEDDING_DIM = 64\n",
        "HIDDEN_DIM = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ssdHUA34TZx-"
      },
      "outputs": [],
      "source": [
        "# create the model\n",
        "class LSTMTagger(nn.Module):\n",
        "\n",
        "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
        "        super(LSTMTagger, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
        "        # with dimensionality hidden_dim.\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "\n",
        "        # The linear layer that maps from hidden state space to tag space\n",
        "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, sentence):\n",
        "        embeds = self.word_embeddings(sentence)\n",
        "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
        "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
        "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
        "        return tag_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z40clRaPTZyA",
        "outputId": "a4d0f437-9750-4791-8f93-85d472db8576"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, training_loss 8289.374361423128\n",
            "epoch: 1, training_loss 4861.855665402698\n",
            "epoch: 2, training_loss 3578.678978038055\n",
            "epoch: 3, training_loss 2797.6594345227795\n",
            "epoch: 4, training_loss 2255.451435390626\n",
            "epoch: 5, training_loss 1852.0988434195333\n",
            "epoch: 6, training_loss 1542.8205318482903\n",
            "epoch: 7, training_loss 1296.6075918182537\n",
            "epoch: 8, training_loss 1099.8587979779436\n",
            "epoch: 9, training_loss 937.6252487192874\n",
            "epoch: 10, training_loss 805.5334117588087\n",
            "epoch: 11, training_loss 698.4410927951419\n",
            "epoch: 12, training_loss 611.437348969463\n",
            "epoch: 13, training_loss 541.4484233296681\n",
            "epoch: 14, training_loss 482.978456996154\n",
            "epoch: 15, training_loss 435.24485981858413\n",
            "epoch: 16, training_loss 395.89561472764257\n",
            "epoch: 17, training_loss 363.96372707570316\n",
            "epoch: 18, training_loss 337.2359863629033\n",
            "epoch: 19, training_loss 314.40385166606364\n",
            "epoch: 20, training_loss 295.3816715306792\n",
            "epoch: 21, training_loss 279.3123037045374\n",
            "epoch: 22, training_loss 265.64869915906246\n",
            "epoch: 23, training_loss 253.59903081744\n",
            "epoch: 24, training_loss 242.98728472822427\n",
            "epoch: 25, training_loss 233.7473777903577\n",
            "epoch: 26, training_loss 225.74989622081725\n",
            "epoch: 27, training_loss 218.7698655582178\n",
            "epoch: 28, training_loss 212.52125194744866\n",
            "epoch: 29, training_loss 207.26351518783034\n"
          ]
        }
      ],
      "source": [
        "# train the model\n",
        "LSTM_model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
        "loss_function = nn.NLLLoss()\n",
        "optimizer = optim.SGD(LSTM_model.parameters(), lr=0.1)\n",
        "\n",
        "# See what the scores are before training\n",
        "# Note that element i,j of the output is the score for tag j for word i.\n",
        "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
        "with torch.no_grad():\n",
        "    inputs = prepare_sequence(training_data[1][0], word_to_ix)\n",
        "    tag_scores = LSTM_model(inputs)\n",
        "    # print(tag_scores)\n",
        "\n",
        "training_losses = []\n",
        "for epoch in range(30):  # again, normally you would NOT do 300 epochs, it is toy data\n",
        "    training_loss = 0.0\n",
        "    for sentence, tags in training_data:\n",
        "        # Step 1. Remember that Pytorch accumulates gradients.\n",
        "        # We need to clear them out before each instance\n",
        "        LSTM_model.zero_grad()\n",
        "\n",
        "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
        "        # Tensors of word indices.\n",
        "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
        "        targets = prepare_sequence(tags, tag_to_ix)\n",
        "\n",
        "        # Step 3. Run our forward pass.\n",
        "        tag_scores = LSTM_model(sentence_in)\n",
        "\n",
        "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "        #  calling optimizer.step()\n",
        "        loss = loss_function(tag_scores, targets)\n",
        "        training_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    training_losses.append(training_loss)\n",
        "    print(f'epoch: {epoch}, training_loss {training_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8j8K4JS3TZyB"
      },
      "source": [
        "### 2.2 Transformer Tagger\n",
        "    We will also use Transformer to solve the NER problem. You can refer to the following link to implement\n",
        "    your Transformer tagger: https://pytorch.org/tutorials/beginner/transformer_tutorial.html."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WDLPfLgXTZyC"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Tuple\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "from torch.utils.data import dataset\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "\n",
        "    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,\n",
        "                 nlayers: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.model_type = 'Transformer'\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "        self.encoder = nn.Embedding(ntoken, d_model)\n",
        "        self.d_model = d_model\n",
        "        self.decoder = nn.Linear(d_model, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self) -> None:\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src: Tensor, src_mask: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src: Tensor, shape [seq_len, batch_size]\n",
        "            src_mask: Tensor, shape [seq_len, seq_len]\n",
        "\n",
        "        Returns:\n",
        "            output Tensor of shape [seq_len, batch_size, ntoken]\n",
        "        \"\"\"\n",
        "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.decoder(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
        "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
        "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GP2mRtkgTZyD"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "qBbgUQasTZyD"
      },
      "outputs": [],
      "source": [
        "# %pip install torchdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "_gdvy3HUTZyE"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "# train_iter = WikiText2(split='train')\n",
        "# type(train_iter)\n",
        "train_iter = [' '.join(sentence) for sentence, _ in train]\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "vocab = build_vocab_from_iterator(map(tokenizer, train_iter), specials=['<unk>'])\n",
        "# vocab = build_vocab_from_iterator(train_iter, specials=['<unk>'])\n",
        "vocab.set_default_index(vocab['<unk>'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rmlRyZXHDpH5",
        "outputId": "80863313-f426-47a4-9e88-479b0f5362d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "j3bnaOPaTZyE"
      },
      "outputs": [],
      "source": [
        "from torchtext.datasets import WikiText2\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "\n",
        "\n",
        "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
        "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
        "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
        "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
        "\n",
        "# train_iter was \"consumed\" by the process of building the vocab,\n",
        "# so we have to create it again\n",
        "# train_iter, val_iter, test_iter = WikiText2()\n",
        "train_iter = [' '.join(sentence) for sentence, _ in train]\n",
        "val_iter = [' '.join(sentence) for sentence, _ in valid]\n",
        "test_iter = [' '.join(sentence) for sentence, _ in test]\n",
        "\n",
        "train_data = data_process(train_iter)\n",
        "val_data = data_process(val_iter)\n",
        "test_data = data_process(test_iter)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
        "    \"\"\"Divides the data into bsz separate sequences, removing extra elements\n",
        "    that wouldn't cleanly fit.\n",
        "\n",
        "    Args:\n",
        "        data: Tensor, shape [N]\n",
        "        bsz: int, batch size\n",
        "\n",
        "    Returns:\n",
        "        Tensor of shape [N // bsz, bsz]\n",
        "    \"\"\"\n",
        "    seq_len = data.size(0) // bsz\n",
        "    data = data[:seq_len * bsz]\n",
        "    data = data.view(bsz, seq_len).t().contiguous()\n",
        "    return data.to(device)\n",
        "\n",
        "batch_size = 20\n",
        "eval_batch_size = 10\n",
        "train_data = batchify(train_data, batch_size)  # shape [seq_len, batch_size]\n",
        "val_data = batchify(val_data, eval_batch_size)\n",
        "test_data = batchify(test_data, eval_batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "J9jKTce7TZyF"
      },
      "outputs": [],
      "source": [
        "bptt = 35\n",
        "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        source: Tensor, shape [full_seq_len, batch_size]\n",
        "        i: int\n",
        "\n",
        "    Returns:\n",
        "        tuple (data, target), where data has shape [seq_len, batch_size] and\n",
        "        target has shape [seq_len * batch_size]\n",
        "    \"\"\"\n",
        "    seq_len = min(bptt, len(source) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
        "    return data, target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "d85bwS32TZyF"
      },
      "outputs": [],
      "source": [
        "ntokens = len(vocab)  # size of vocabulary\n",
        "emsize = 200  # embedding dimension\n",
        "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
        "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
        "nhead = 2  # number of heads in nn.MultiheadAttention\n",
        "dropout = 0.2  # dropout probability\n",
        "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "Ofp08NAPTZyG"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "lr = 5.0  # learning rate\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
        "\n",
        "def train(model: nn.Module) -> None:\n",
        "    model.train()  # turn on train mode\n",
        "    total_loss = 0.\n",
        "    log_interval = 200\n",
        "    start_time = time.time()\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "\n",
        "    num_batches = len(train_data) // bptt\n",
        "    for batch, i in enumerate(range(0, train_data.size(0) - 1, bptt)):\n",
        "        data, targets = get_batch(train_data, i)\n",
        "        seq_len = data.size(0)\n",
        "        if seq_len != bptt:  # only on last batch\n",
        "            src_mask = src_mask[:seq_len, :seq_len]\n",
        "        output = model(data, src_mask)\n",
        "        loss = criterion(output.view(-1, ntokens), targets)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        if batch % log_interval == 0 and batch > 0:\n",
        "            lr = scheduler.get_last_lr()[0]\n",
        "            ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
        "            cur_loss = total_loss / log_interval\n",
        "            ppl = math.exp(cur_loss)\n",
        "            print(f'| epoch {epoch:3d} | {batch:5d}/{num_batches:5d} batches | '\n",
        "                  f'lr {lr:02.2f} | ms/batch {ms_per_batch:5.2f} | '\n",
        "                  f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
        "            total_loss = 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(model: nn.Module, eval_data: Tensor) -> float:\n",
        "    model.eval()  # turn on evaluation mode\n",
        "    total_loss = 0.\n",
        "    src_mask = generate_square_subsequent_mask(bptt).to(device)\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, eval_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(eval_data, i)\n",
        "            seq_len = data.size(0)\n",
        "            if seq_len != bptt:\n",
        "                src_mask = src_mask[:seq_len, :seq_len]\n",
        "            output = model(data, src_mask)\n",
        "            output_flat = output.view(-1, ntokens)\n",
        "            total_loss += seq_len * criterion(output_flat, targets).item()\n",
        "    return total_loss / (len(eval_data) - 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "SwsDBVyZTZyG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "748235c0-e1d6-4e8f-ffcc-30025e215efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   200/  303 batches | lr 5.00 | ms/batch 23.19 | loss  8.03 | ppl  3084.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   1 | time:  6.10s | valid loss  6.28 | valid ppl   532.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   2 |   200/  303 batches | lr 4.75 | ms/batch 10.54 | loss  6.45 | ppl   635.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   2 | time:  3.58s | valid loss  5.87 | valid ppl   355.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   3 |   200/  303 batches | lr 4.51 | ms/batch 10.61 | loss  5.89 | ppl   362.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   3 | time:  3.60s | valid loss  5.82 | valid ppl   338.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   4 |   200/  303 batches | lr 4.29 | ms/batch 10.63 | loss  5.47 | ppl   237.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   4 | time:  3.61s | valid loss  5.75 | valid ppl   314.39\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   5 |   200/  303 batches | lr 4.07 | ms/batch 10.64 | loss  5.11 | ppl   165.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   5 | time:  3.60s | valid loss  5.78 | valid ppl   323.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   6 |   200/  303 batches | lr 3.87 | ms/batch 10.67 | loss  4.79 | ppl   120.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   6 | time:  3.75s | valid loss  5.89 | valid ppl   361.45\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   7 |   200/  303 batches | lr 3.68 | ms/batch 10.96 | loss  4.52 | ppl    91.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   7 | time:  3.70s | valid loss  5.82 | valid ppl   336.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   8 |   200/  303 batches | lr 3.49 | ms/batch 10.79 | loss  4.27 | ppl    71.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   8 | time:  3.64s | valid loss  5.91 | valid ppl   366.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch   9 |   200/  303 batches | lr 3.32 | ms/batch 10.71 | loss  4.06 | ppl    58.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch   9 | time:  3.63s | valid loss  6.07 | valid ppl   431.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  10 |   200/  303 batches | lr 3.15 | ms/batch 10.73 | loss  3.89 | ppl    48.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  10 | time:  3.63s | valid loss  6.01 | valid ppl   405.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  11 |   200/  303 batches | lr 2.99 | ms/batch 10.74 | loss  3.73 | ppl    41.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  11 | time:  3.63s | valid loss  6.10 | valid ppl   444.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  12 |   200/  303 batches | lr 2.84 | ms/batch 10.83 | loss  3.60 | ppl    36.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  12 | time:  3.65s | valid loss  6.18 | valid ppl   483.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  13 |   200/  303 batches | lr 2.70 | ms/batch 10.80 | loss  3.48 | ppl    32.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  13 | time:  3.65s | valid loss  6.33 | valid ppl   560.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  14 |   200/  303 batches | lr 2.57 | ms/batch 10.82 | loss  3.37 | ppl    29.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  14 | time:  3.66s | valid loss  6.30 | valid ppl   543.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  15 |   200/  303 batches | lr 2.44 | ms/batch 10.81 | loss  3.27 | ppl    26.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  15 | time:  3.66s | valid loss  6.27 | valid ppl   528.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  16 |   200/  303 batches | lr 2.32 | ms/batch 10.94 | loss  3.18 | ppl    24.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  16 | time:  3.69s | valid loss  6.36 | valid ppl   580.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  17 |   200/  303 batches | lr 2.20 | ms/batch 10.87 | loss  3.08 | ppl    21.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  17 | time:  3.67s | valid loss  6.47 | valid ppl   647.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  18 |   200/  303 batches | lr 2.09 | ms/batch 10.91 | loss  3.01 | ppl    20.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  18 | time:  3.68s | valid loss  6.52 | valid ppl   676.34\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  19 |   200/  303 batches | lr 1.99 | ms/batch 10.89 | loss  2.92 | ppl    18.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  19 | time:  3.69s | valid loss  6.64 | valid ppl   765.38\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  20 |   200/  303 batches | lr 1.89 | ms/batch 10.92 | loss  2.85 | ppl    17.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  20 | time:  3.69s | valid loss  6.63 | valid ppl   759.88\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  21 |   200/  303 batches | lr 1.79 | ms/batch 11.01 | loss  2.78 | ppl    16.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  21 | time:  3.72s | valid loss  6.69 | valid ppl   801.55\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  22 |   200/  303 batches | lr 1.70 | ms/batch 11.03 | loss  2.71 | ppl    14.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  22 | time:  3.71s | valid loss  6.70 | valid ppl   813.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  23 |   200/  303 batches | lr 1.62 | ms/batch 11.43 | loss  2.64 | ppl    14.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  23 | time:  3.81s | valid loss  6.78 | valid ppl   882.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  24 |   200/  303 batches | lr 1.54 | ms/batch 11.02 | loss  2.58 | ppl    13.22\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  24 | time:  3.71s | valid loss  6.81 | valid ppl   909.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  25 |   200/  303 batches | lr 1.46 | ms/batch 10.99 | loss  2.52 | ppl    12.41\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  25 | time:  3.71s | valid loss  6.87 | valid ppl   964.07\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  26 |   200/  303 batches | lr 1.39 | ms/batch 11.03 | loss  2.47 | ppl    11.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  26 | time:  3.71s | valid loss  6.86 | valid ppl   949.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  27 |   200/  303 batches | lr 1.32 | ms/batch 11.03 | loss  2.41 | ppl    11.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  27 | time:  3.72s | valid loss  6.95 | valid ppl  1047.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  28 |   200/  303 batches | lr 1.25 | ms/batch 11.03 | loss  2.36 | ppl    10.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  28 | time:  3.72s | valid loss  7.00 | valid ppl  1098.91\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  29 |   200/  303 batches | lr 1.19 | ms/batch 11.08 | loss  2.32 | ppl    10.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  29 | time:  3.73s | valid loss  7.02 | valid ppl  1113.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  30 |   200/  303 batches | lr 1.13 | ms/batch 11.07 | loss  2.27 | ppl     9.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  30 | time:  3.74s | valid loss  7.08 | valid ppl  1192.35\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  31 |   200/  303 batches | lr 1.07 | ms/batch 11.09 | loss  2.23 | ppl     9.26\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  31 | time:  3.74s | valid loss  7.10 | valid ppl  1211.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  32 |   200/  303 batches | lr 1.02 | ms/batch 11.18 | loss  2.19 | ppl     8.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  32 | time:  3.76s | valid loss  7.16 | valid ppl  1292.96\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  33 |   200/  303 batches | lr 0.97 | ms/batch 11.14 | loss  2.15 | ppl     8.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  33 | time:  3.75s | valid loss  7.22 | valid ppl  1359.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  34 |   200/  303 batches | lr 0.92 | ms/batch 11.18 | loss  2.12 | ppl     8.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  34 | time:  3.75s | valid loss  7.25 | valid ppl  1412.30\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  35 |   200/  303 batches | lr 0.87 | ms/batch 11.15 | loss  2.08 | ppl     8.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  35 | time:  3.75s | valid loss  7.29 | valid ppl  1465.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  36 |   200/  303 batches | lr 0.83 | ms/batch 11.14 | loss  2.05 | ppl     7.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  36 | time:  3.76s | valid loss  7.32 | valid ppl  1505.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  37 |   200/  303 batches | lr 0.79 | ms/batch 11.18 | loss  2.02 | ppl     7.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  37 | time:  3.77s | valid loss  7.34 | valid ppl  1541.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  38 |   200/  303 batches | lr 0.75 | ms/batch 11.21 | loss  2.00 | ppl     7.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  38 | time:  3.76s | valid loss  7.36 | valid ppl  1570.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  39 |   200/  303 batches | lr 0.71 | ms/batch 11.65 | loss  1.97 | ppl     7.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  39 | time:  4.13s | valid loss  7.38 | valid ppl  1602.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  40 |   200/  303 batches | lr 0.68 | ms/batch 11.54 | loss  1.95 | ppl     7.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  40 | time:  3.85s | valid loss  7.39 | valid ppl  1617.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  41 |   200/  303 batches | lr 0.64 | ms/batch 11.24 | loss  1.92 | ppl     6.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  41 | time:  3.79s | valid loss  7.44 | valid ppl  1696.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  42 |   200/  303 batches | lr 0.61 | ms/batch 11.27 | loss  1.90 | ppl     6.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  42 | time:  3.80s | valid loss  7.48 | valid ppl  1779.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  43 |   200/  303 batches | lr 0.58 | ms/batch 11.29 | loss  1.88 | ppl     6.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  43 | time:  3.81s | valid loss  7.52 | valid ppl  1850.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  44 |   200/  303 batches | lr 0.55 | ms/batch 11.34 | loss  1.86 | ppl     6.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  44 | time:  3.80s | valid loss  7.53 | valid ppl  1868.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  45 |   200/  303 batches | lr 0.52 | ms/batch 11.35 | loss  1.84 | ppl     6.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  45 | time:  3.82s | valid loss  7.52 | valid ppl  1844.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  46 |   200/  303 batches | lr 0.50 | ms/batch 11.36 | loss  1.82 | ppl     6.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  46 | time:  3.82s | valid loss  7.57 | valid ppl  1934.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  47 |   200/  303 batches | lr 0.47 | ms/batch 11.27 | loss  1.81 | ppl     6.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  47 | time:  3.80s | valid loss  7.58 | valid ppl  1960.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  48 |   200/  303 batches | lr 0.45 | ms/batch 11.32 | loss  1.79 | ppl     6.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  48 | time:  3.82s | valid loss  7.63 | valid ppl  2053.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  49 |   200/  303 batches | lr 0.43 | ms/batch 11.26 | loss  1.78 | ppl     5.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  49 | time:  3.79s | valid loss  7.62 | valid ppl  2046.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  50 |   200/  303 batches | lr 0.40 | ms/batch 11.28 | loss  1.77 | ppl     5.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  50 | time:  3.79s | valid loss  7.64 | valid ppl  2089.13\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  51 |   200/  303 batches | lr 0.38 | ms/batch 11.31 | loss  1.75 | ppl     5.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  51 | time:  3.80s | valid loss  7.64 | valid ppl  2082.48\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  52 |   200/  303 batches | lr 0.37 | ms/batch 11.32 | loss  1.74 | ppl     5.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  52 | time:  3.82s | valid loss  7.68 | valid ppl  2155.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  53 |   200/  303 batches | lr 0.35 | ms/batch 11.32 | loss  1.73 | ppl     5.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  53 | time:  3.81s | valid loss  7.69 | valid ppl  2181.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  54 |   200/  303 batches | lr 0.33 | ms/batch 11.38 | loss  1.72 | ppl     5.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  54 | time:  3.84s | valid loss  7.68 | valid ppl  2158.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  55 |   200/  303 batches | lr 0.31 | ms/batch 11.44 | loss  1.72 | ppl     5.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  55 | time:  3.99s | valid loss  7.69 | valid ppl  2196.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  56 |   200/  303 batches | lr 0.30 | ms/batch 11.70 | loss  1.70 | ppl     5.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  56 | time:  3.93s | valid loss  7.70 | valid ppl  2214.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  57 |   200/  303 batches | lr 0.28 | ms/batch 11.49 | loss  1.69 | ppl     5.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  57 | time:  3.85s | valid loss  7.71 | valid ppl  2226.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  58 |   200/  303 batches | lr 0.27 | ms/batch 11.45 | loss  1.68 | ppl     5.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  58 | time:  3.85s | valid loss  7.73 | valid ppl  2275.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  59 |   200/  303 batches | lr 0.26 | ms/batch 11.43 | loss  1.67 | ppl     5.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  59 | time:  3.84s | valid loss  7.73 | valid ppl  2282.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  60 |   200/  303 batches | lr 0.24 | ms/batch 11.42 | loss  1.67 | ppl     5.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  60 | time:  3.85s | valid loss  7.72 | valid ppl  2260.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  61 |   200/  303 batches | lr 0.23 | ms/batch 11.41 | loss  1.66 | ppl     5.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  61 | time:  3.83s | valid loss  7.73 | valid ppl  2270.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  62 |   200/  303 batches | lr 0.22 | ms/batch 11.42 | loss  1.65 | ppl     5.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  62 | time:  3.84s | valid loss  7.75 | valid ppl  2319.23\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  63 |   200/  303 batches | lr 0.21 | ms/batch 11.42 | loss  1.65 | ppl     5.19\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  63 | time:  3.84s | valid loss  7.75 | valid ppl  2329.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  64 |   200/  303 batches | lr 0.20 | ms/batch 11.40 | loss  1.64 | ppl     5.18\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  64 | time:  3.83s | valid loss  7.76 | valid ppl  2346.09\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  65 |   200/  303 batches | lr 0.19 | ms/batch 11.37 | loss  1.64 | ppl     5.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  65 | time:  3.83s | valid loss  7.76 | valid ppl  2354.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  66 |   200/  303 batches | lr 0.18 | ms/batch 11.36 | loss  1.63 | ppl     5.12\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  66 | time:  3.82s | valid loss  7.76 | valid ppl  2355.20\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  67 |   200/  303 batches | lr 0.17 | ms/batch 11.35 | loss  1.63 | ppl     5.08\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  67 | time:  3.82s | valid loss  7.78 | valid ppl  2401.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  68 |   200/  303 batches | lr 0.16 | ms/batch 11.37 | loss  1.62 | ppl     5.06\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  68 | time:  3.83s | valid loss  7.78 | valid ppl  2380.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  69 |   200/  303 batches | lr 0.15 | ms/batch 11.39 | loss  1.62 | ppl     5.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  69 | time:  3.82s | valid loss  7.78 | valid ppl  2401.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  70 |   200/  303 batches | lr 0.15 | ms/batch 11.35 | loss  1.61 | ppl     5.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  70 | time:  3.83s | valid loss  7.79 | valid ppl  2419.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  71 |   200/  303 batches | lr 0.14 | ms/batch 11.40 | loss  1.61 | ppl     4.98\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  71 | time:  3.97s | valid loss  7.78 | valid ppl  2399.03\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  72 |   200/  303 batches | lr 0.13 | ms/batch 11.56 | loss  1.61 | ppl     5.00\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  72 | time:  3.86s | valid loss  7.79 | valid ppl  2419.10\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  73 |   200/  303 batches | lr 0.12 | ms/batch 11.36 | loss  1.60 | ppl     4.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  73 | time:  3.83s | valid loss  7.80 | valid ppl  2449.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  74 |   200/  303 batches | lr 0.12 | ms/batch 11.40 | loss  1.60 | ppl     4.94\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  74 | time:  3.83s | valid loss  7.80 | valid ppl  2431.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  75 |   200/  303 batches | lr 0.11 | ms/batch 11.34 | loss  1.59 | ppl     4.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  75 | time:  3.83s | valid loss  7.80 | valid ppl  2439.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  76 |   200/  303 batches | lr 0.11 | ms/batch 11.40 | loss  1.59 | ppl     4.92\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  76 | time:  3.83s | valid loss  7.80 | valid ppl  2434.93\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  77 |   200/  303 batches | lr 0.10 | ms/batch 11.45 | loss  1.59 | ppl     4.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  77 | time:  3.85s | valid loss  7.82 | valid ppl  2479.31\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  78 |   200/  303 batches | lr 0.10 | ms/batch 11.43 | loss  1.58 | ppl     4.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  78 | time:  3.84s | valid loss  7.80 | valid ppl  2446.17\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  79 |   200/  303 batches | lr 0.09 | ms/batch 11.40 | loss  1.58 | ppl     4.86\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  79 | time:  3.87s | valid loss  7.81 | valid ppl  2465.84\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  80 |   200/  303 batches | lr 0.09 | ms/batch 11.50 | loss  1.58 | ppl     4.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  80 | time:  3.88s | valid loss  7.81 | valid ppl  2474.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  81 |   200/  303 batches | lr 0.08 | ms/batch 11.43 | loss  1.58 | ppl     4.83\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  81 | time:  3.84s | valid loss  7.81 | valid ppl  2458.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  82 |   200/  303 batches | lr 0.08 | ms/batch 11.43 | loss  1.58 | ppl     4.85\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  82 | time:  3.84s | valid loss  7.82 | valid ppl  2487.28\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  83 |   200/  303 batches | lr 0.07 | ms/batch 11.40 | loss  1.57 | ppl     4.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  83 | time:  3.83s | valid loss  7.81 | valid ppl  2474.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  84 |   200/  303 batches | lr 0.07 | ms/batch 11.43 | loss  1.57 | ppl     4.81\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  84 | time:  3.85s | valid loss  7.82 | valid ppl  2481.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  85 |   200/  303 batches | lr 0.07 | ms/batch 11.40 | loss  1.57 | ppl     4.80\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  85 | time:  3.84s | valid loss  7.82 | valid ppl  2478.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  86 |   200/  303 batches | lr 0.06 | ms/batch 11.41 | loss  1.57 | ppl     4.79\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  86 | time:  3.83s | valid loss  7.81 | valid ppl  2468.87\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  87 |   200/  303 batches | lr 0.06 | ms/batch 11.43 | loss  1.56 | ppl     4.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  87 | time:  3.96s | valid loss  7.82 | valid ppl  2483.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  88 |   200/  303 batches | lr 0.06 | ms/batch 11.69 | loss  1.56 | ppl     4.78\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  88 | time:  3.90s | valid loss  7.82 | valid ppl  2492.37\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  89 |   200/  303 batches | lr 0.05 | ms/batch 11.41 | loss  1.56 | ppl     4.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  89 | time:  3.84s | valid loss  7.83 | valid ppl  2502.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  90 |   200/  303 batches | lr 0.05 | ms/batch 11.43 | loss  1.56 | ppl     4.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  90 | time:  3.83s | valid loss  7.83 | valid ppl  2521.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  91 |   200/  303 batches | lr 0.05 | ms/batch 11.41 | loss  1.56 | ppl     4.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  91 | time:  3.83s | valid loss  7.82 | valid ppl  2499.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  92 |   200/  303 batches | lr 0.05 | ms/batch 11.39 | loss  1.56 | ppl     4.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  92 | time:  3.83s | valid loss  7.82 | valid ppl  2500.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  93 |   200/  303 batches | lr 0.04 | ms/batch 11.38 | loss  1.55 | ppl     4.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  93 | time:  3.83s | valid loss  7.83 | valid ppl  2526.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  94 |   200/  303 batches | lr 0.04 | ms/batch 11.38 | loss  1.55 | ppl     4.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  94 | time:  3.82s | valid loss  7.84 | valid ppl  2533.05\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  95 |   200/  303 batches | lr 0.04 | ms/batch 11.38 | loss  1.55 | ppl     4.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  95 | time:  3.83s | valid loss  7.83 | valid ppl  2525.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  96 |   200/  303 batches | lr 0.04 | ms/batch 11.44 | loss  1.55 | ppl     4.72\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  96 | time:  3.84s | valid loss  7.84 | valid ppl  2536.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  97 |   200/  303 batches | lr 0.04 | ms/batch 11.40 | loss  1.55 | ppl     4.70\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  97 | time:  3.85s | valid loss  7.84 | valid ppl  2544.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  98 |   200/  303 batches | lr 0.03 | ms/batch 11.38 | loss  1.55 | ppl     4.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  98 | time:  3.83s | valid loss  7.84 | valid ppl  2539.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch  99 |   200/  303 batches | lr 0.03 | ms/batch 11.38 | loss  1.55 | ppl     4.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch  99 | time:  3.84s | valid loss  7.85 | valid ppl  2553.50\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 100 |   200/  303 batches | lr 0.03 | ms/batch 11.41 | loss  1.54 | ppl     4.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 100 | time:  3.83s | valid loss  7.84 | valid ppl  2542.33\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 101 |   200/  303 batches | lr 0.03 | ms/batch 11.36 | loss  1.55 | ppl     4.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 101 | time:  3.91s | valid loss  7.85 | valid ppl  2564.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 102 |   200/  303 batches | lr 0.03 | ms/batch 11.71 | loss  1.55 | ppl     4.71\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 102 | time:  3.90s | valid loss  7.85 | valid ppl  2568.11\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 103 |   200/  303 batches | lr 0.03 | ms/batch 11.39 | loss  1.54 | ppl     4.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 103 | time:  3.94s | valid loss  7.84 | valid ppl  2548.32\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 104 |   200/  303 batches | lr 0.03 | ms/batch 11.74 | loss  1.54 | ppl     4.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 104 | time:  3.91s | valid loss  7.85 | valid ppl  2565.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 105 |   200/  303 batches | lr 0.02 | ms/batch 11.39 | loss  1.55 | ppl     4.69\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 105 | time:  3.83s | valid loss  7.85 | valid ppl  2568.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 106 |   200/  303 batches | lr 0.02 | ms/batch 11.41 | loss  1.54 | ppl     4.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 106 | time:  3.85s | valid loss  7.85 | valid ppl  2573.25\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 107 |   200/  303 batches | lr 0.02 | ms/batch 11.39 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 107 | time:  3.84s | valid loss  7.85 | valid ppl  2573.82\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 108 |   200/  303 batches | lr 0.02 | ms/batch 11.40 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 108 | time:  3.83s | valid loss  7.86 | valid ppl  2580.97\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 109 |   200/  303 batches | lr 0.02 | ms/batch 11.41 | loss  1.54 | ppl     4.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 109 | time:  3.84s | valid loss  7.86 | valid ppl  2583.01\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 110 |   200/  303 batches | lr 0.02 | ms/batch 11.38 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 110 | time:  3.83s | valid loss  7.86 | valid ppl  2589.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 111 |   200/  303 batches | lr 0.02 | ms/batch 11.37 | loss  1.54 | ppl     4.67\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 111 | time:  3.82s | valid loss  7.86 | valid ppl  2582.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 112 |   200/  303 batches | lr 0.02 | ms/batch 11.42 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 112 | time:  3.85s | valid loss  7.86 | valid ppl  2580.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 113 |   200/  303 batches | lr 0.02 | ms/batch 11.39 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 113 | time:  3.83s | valid loss  7.86 | valid ppl  2595.36\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 114 |   200/  303 batches | lr 0.02 | ms/batch 11.37 | loss  1.53 | ppl     4.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 114 | time:  3.84s | valid loss  7.86 | valid ppl  2604.02\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 115 |   200/  303 batches | lr 0.01 | ms/batch 11.44 | loss  1.54 | ppl     4.68\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 115 | time:  3.85s | valid loss  7.86 | valid ppl  2597.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 116 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.54 | ppl     4.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 116 | time:  3.85s | valid loss  7.86 | valid ppl  2599.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 117 |   200/  303 batches | lr 0.01 | ms/batch 11.37 | loss  1.53 | ppl     4.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 117 | time:  3.82s | valid loss  7.86 | valid ppl  2596.52\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 118 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.53 | ppl     4.64\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 118 | time:  3.83s | valid loss  7.86 | valid ppl  2602.51\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 119 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.53 | ppl     4.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 119 | time:  3.95s | valid loss  7.86 | valid ppl  2598.40\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 120 |   200/  303 batches | lr 0.01 | ms/batch 11.76 | loss  1.53 | ppl     4.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 120 | time:  3.91s | valid loss  7.86 | valid ppl  2599.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 121 |   200/  303 batches | lr 0.01 | ms/batch 11.37 | loss  1.53 | ppl     4.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 121 | time:  3.84s | valid loss  7.86 | valid ppl  2596.44\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 122 |   200/  303 batches | lr 0.01 | ms/batch 11.41 | loss  1.53 | ppl     4.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 122 | time:  3.83s | valid loss  7.87 | valid ppl  2605.14\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 123 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 123 | time:  3.83s | valid loss  7.86 | valid ppl  2603.75\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 124 |   200/  303 batches | lr 0.01 | ms/batch 11.43 | loss  1.53 | ppl     4.62\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 124 | time:  3.85s | valid loss  7.86 | valid ppl  2603.76\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 125 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 125 | time:  3.84s | valid loss  7.87 | valid ppl  2606.53\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 126 |   200/  303 batches | lr 0.01 | ms/batch 11.43 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 126 | time:  3.84s | valid loss  7.87 | valid ppl  2610.66\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 127 |   200/  303 batches | lr 0.01 | ms/batch 11.41 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 127 | time:  3.85s | valid loss  7.87 | valid ppl  2612.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 128 |   200/  303 batches | lr 0.01 | ms/batch 11.45 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 128 | time:  3.84s | valid loss  7.87 | valid ppl  2616.63\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 129 |   200/  303 batches | lr 0.01 | ms/batch 11.41 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 129 | time:  3.83s | valid loss  7.87 | valid ppl  2614.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 130 |   200/  303 batches | lr 0.01 | ms/batch 11.40 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 130 | time:  3.85s | valid loss  7.87 | valid ppl  2612.73\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 131 |   200/  303 batches | lr 0.01 | ms/batch 11.44 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 131 | time:  3.84s | valid loss  7.87 | valid ppl  2612.74\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 132 |   200/  303 batches | lr 0.01 | ms/batch 11.36 | loss  1.52 | ppl     4.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 132 | time:  3.83s | valid loss  7.87 | valid ppl  2612.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 133 |   200/  303 batches | lr 0.01 | ms/batch 11.36 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 133 | time:  3.82s | valid loss  7.87 | valid ppl  2619.65\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 134 |   200/  303 batches | lr 0.01 | ms/batch 11.37 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 134 | time:  3.83s | valid loss  7.87 | valid ppl  2620.16\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 135 |   200/  303 batches | lr 0.01 | ms/batch 11.42 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 135 | time:  3.96s | valid loss  7.87 | valid ppl  2620.90\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 136 |   200/  303 batches | lr 0.00 | ms/batch 11.65 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 136 | time:  3.88s | valid loss  7.87 | valid ppl  2620.46\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 137 |   200/  303 batches | lr 0.00 | ms/batch 11.38 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 137 | time:  3.83s | valid loss  7.87 | valid ppl  2620.27\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 138 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 138 | time:  3.83s | valid loss  7.87 | valid ppl  2620.99\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 139 |   200/  303 batches | lr 0.00 | ms/batch 11.41 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 139 | time:  3.83s | valid loss  7.87 | valid ppl  2622.21\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 140 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.52 | ppl     4.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 140 | time:  3.83s | valid loss  7.87 | valid ppl  2623.29\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 141 |   200/  303 batches | lr 0.00 | ms/batch 11.38 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 141 | time:  3.84s | valid loss  7.87 | valid ppl  2624.56\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 142 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 142 | time:  3.84s | valid loss  7.87 | valid ppl  2622.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 143 |   200/  303 batches | lr 0.00 | ms/batch 11.42 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 143 | time:  3.84s | valid loss  7.87 | valid ppl  2622.04\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 144 |   200/  303 batches | lr 0.00 | ms/batch 11.45 | loss  1.52 | ppl     4.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 144 | time:  3.84s | valid loss  7.87 | valid ppl  2621.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 145 |   200/  303 batches | lr 0.00 | ms/batch 11.41 | loss  1.52 | ppl     4.57\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 145 | time:  3.83s | valid loss  7.87 | valid ppl  2617.89\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 146 |   200/  303 batches | lr 0.00 | ms/batch 11.42 | loss  1.53 | ppl     4.61\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 146 | time:  3.85s | valid loss  7.87 | valid ppl  2617.15\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 147 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.52 | ppl     4.58\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 147 | time:  3.85s | valid loss  7.87 | valid ppl  2617.47\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 148 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 148 | time:  3.84s | valid loss  7.87 | valid ppl  2618.24\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 149 |   200/  303 batches | lr 0.00 | ms/batch 11.40 | loss  1.53 | ppl     4.60\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 149 | time:  3.83s | valid loss  7.87 | valid ppl  2619.42\n",
            "-----------------------------------------------------------------------------------------\n",
            "| epoch 150 |   200/  303 batches | lr 0.00 | ms/batch 11.43 | loss  1.52 | ppl     4.59\n",
            "-----------------------------------------------------------------------------------------\n",
            "| end of epoch 150 | time:  3.83s | valid loss  7.87 | valid ppl  2618.90\n",
            "-----------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "best_val_loss = float('inf')\n",
        "epochs = 150\n",
        "best_model = None\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(model)\n",
        "    val_loss = evaluate(model, val_data)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    elapsed = time.time() - epoch_start_time\n",
        "    print('-' * 89)\n",
        "    print(f'| end of epoch {epoch:3d} | time: {elapsed:5.2f}s | '\n",
        "          f'valid loss {val_loss:5.2f} | valid ppl {val_ppl:8.2f}')\n",
        "    print('-' * 89)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = copy.deepcopy(model)\n",
        "\n",
        "    scheduler.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "2blXuHQQTZyH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75baf6ff-0415-456c-c048-53598dd59d6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========================================================================================\n",
            "| End of training | test loss  5.96 | test ppl   388.18\n",
            "=========================================================================================\n"
          ]
        }
      ],
      "source": [
        "test_loss = evaluate(best_model, test_data)\n",
        "test_ppl = math.exp(test_loss)\n",
        "print('=' * 89)\n",
        "print(f'| End of training | test loss {test_loss:5.2f} | '\n",
        "      f'test ppl {test_ppl:8.2f}')\n",
        "print('=' * 89)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "-Fxl8JRTDpH8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKBIPZoKTZyH"
      },
      "source": [
        "### 2.3 Results\n",
        "    Please report your best performance on the test set in the same format as the test.txt file, but replace the ground truth labels with your prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "Ls-wErWV4lMi"
      },
      "outputs": [],
      "source": [
        "def LSTM_calculate_accuracy(model, data, tags, words):\n",
        "    \n",
        "    # for s in range(len(data)):\n",
        "    #     # print(data[s])\n",
        "    #     for w in range(len(data[s][0])):\n",
        "    #         # print(data[s][0][w])\n",
        "    #         if data[s][0][w] not in words:  # word has not been assigned an index yet\n",
        "    #             data[s][0][w] = '.'\n",
        "\n",
        "    correct = 0\n",
        "    # print(tags)\n",
        "\n",
        "    pred_tags = []\n",
        "    with torch.no_grad():\n",
        "        for x, y in data:\n",
        "            inputs = prepare_sequence(x, words)\n",
        "            tag_scores = model(inputs)\n",
        "            # print(x)\n",
        "\n",
        "            idx = np.argmax(tag_scores, axis=1)\n",
        "            # print(idx)\n",
        "            pred_tag = [tags[i] for i in idx]\n",
        "            # print(\"predicted result: \", pred_tag)\n",
        "            # print(\"ground truth:     \", y)\n",
        "            pred_tags.append(pred_tag)\n",
        "\n",
        "            correct += sum([y==pred_tag])\n",
        "    \n",
        "    return correct/len(data), pred_tags"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(tag_to_ix.keys())\n",
        "tags = np.array([tag for tag in tag_to_ix])\n",
        "\n",
        "acc_train, pred_train = LSTM_calculate_accuracy(LSTM_model, train, tags, word_to_ix)\n",
        "print(\"accuracy of the LSTM model in train data: \", acc_train)\n",
        "\n",
        "acc_valid, pred_valid = LSTM_calculate_accuracy(LSTM_model, valid, tags, word_to_ix)\n",
        "print(\"accuracy of the LSTM model in valid data: \", acc_valid)\n",
        "\n",
        "acc_test, pred_test = LSTM_calculate_accuracy(LSTM_model, test, tags, word_to_ix)\n",
        "print(\"accuracy of the LSTM model in test data: \", acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "338FWwM54xd1",
        "outputId": "4e0bd862-b12e-4121-819f-462c8d223993"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy of the LSTM model in train data:  0.958297190898779\n",
            "accuracy of the LSTM model in valid data:  0.536064627813041\n",
            "accuracy of the LSTM model in test data:  0.46199782844733983\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "y-KNY7wdTZyH"
      },
      "outputs": [],
      "source": [
        "def LSTM_output_pred(pred, input_file, output_file):\n",
        "    import os\n",
        "    import numpy as np\n",
        "\n",
        "    data_dir = os.path.join(os.getcwd(), \"data\")\n",
        "    input_path = os.path.join(data_dir, input_file)\n",
        "    output_path = os.path.join(data_dir, output_file)\n",
        "\n",
        "    pred_list = []\n",
        "    for p in pred:\n",
        "      for q in p:\n",
        "        pred_list.append(q)\n",
        "      pred_list.append('')\n",
        "\n",
        "    with open(input_path, 'r', encoding=\"utf-8\") as f:\n",
        "        input = [l.strip().split(' ') for l in f.readlines()]\n",
        "    f.close()\n",
        "\n",
        "    # print(len(pred_list), len(input))\n",
        "    with open(output_path, 'w', encoding=\"utf-8\") as f:\n",
        "        while len(input) > 0:\n",
        "          # print(len(pred_list), len(input))\n",
        "          output = input.pop(0)\n",
        "          output[-1] = pred_list.pop(0)\n",
        "          # print(output)\n",
        "          f.write(' '.join(output) + \"\\n\")\n",
        "    f.close()\n",
        "\n",
        "    print(f'Result saved in {output_path} successfully.\\n')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "LSTM_output_pred(pred_test, 'test.txt', 'LSTM_pred_test.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2zMvQXcefsu",
        "outputId": "44085ff9-d6db-425d-ddf6-cd281b060d34"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result saved in /content/data/LSTM_pred_test.txt successfully.\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "8j8K4JS3TZyB"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "eecfc76fa3750cf2629c769369211f386c60dfd205d50c8d0a6e4dde78012e31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}