{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI4A-T0z5AU6"
      },
      "source": [
        "# Assignment 1\n",
        "You should submit the **UniversityNumber.ipynb** file and your final prediction file **UniversityNumber.test.out** to moodle. Make sure your code does not use your local files and that the results are reproducible. Before submitting, please **run your notebook and keep all running logs** so that we can check."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEomoMzH5Nf6"
      },
      "source": [
        "## 1 $n$-gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YsSAtTqt7Q8a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n",
            "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n",
            "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
          ]
        }
      ],
      "source": [
        "!wget -O train.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/train.txt\n",
        "!wget -O dev.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/dev.txt\n",
        "!wget -O test.txt https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/lm/test.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElrINWW7oF7"
      },
      "source": [
        "### 1.1 Building vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eawcuVV19kZm"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data(dir): # dir: 'lm' or 'prep', mode: 'train' or 'dev'\n",
        "    # load data\n",
        "    import os\n",
        "\n",
        "    if dir=='lm':\n",
        "        data_dir = os.path.join(os.getcwd(), \"data\", \"lm\")\n",
        "        train_path = os.path.join(data_dir, 'train.txt')\n",
        "        dev_path = os.path.join(data_dir, 'dev.txt')\n",
        "        test_path  = os.path.join(data_dir, 'test.txt')\n",
        "    else :\n",
        "        data_dir = os.path.join(os.getcwd(), \"data\", \"prep\")\n",
        "        train_path = os.path.join(data_dir, 'dev.in')\n",
        "        dev_path = os.path.join(data_dir, 'dev.in')\n",
        "        test_path  = os.path.join(data_dir, 'test.in')\n",
        "\n",
        "    with open(train_path, 'r', encoding=\"utf-8\") as f:\n",
        "        train = [l.strip() for l in f.readlines()]\n",
        "    with open(dev_path, 'r', encoding=\"utf-8\") as f:\n",
        "        dev = [l.strip() for l in f.readlines()]\n",
        "    with open(test_path, 'r', encoding=\"utf-8\") as f:\n",
        "        test = [l.strip() for l in f.readlines()]\n",
        "    \n",
        "    return train, dev, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "q-rNT_QL8Dvt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "def build_vocab(sentences, n):\n",
        "    # add start-of-sentence tokens 〈s〉 and end-of-sentence〈/s〉 tokens.\n",
        "    for sentence in sentences:\n",
        "        if n > 1:\n",
        "            sentence = \"<s> \"*(n-1) + sentence + \" </s>\"\n",
        "        else:\n",
        "            sentence = \"<s>\" + sentence + \"</s>\"\n",
        "    \n",
        "    # tokenize training data\n",
        "    tokens = ' '.join(sentences).split(' ')\n",
        " \n",
        "    # create a frequency list\n",
        "    freq = nltk.FreqDist(tokens)\n",
        "\n",
        "    # show the vocabulary size\n",
        "    print(f'vocabulary size: {len(freq)}')\n",
        "\n",
        "   # convert tokens that occur less than three times in the training data into a special unknown token\n",
        "    return [(\"<UNK>\" if freq[token] < 3 else token) for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size: 52640\n",
            "vocabulary size: 19546\n",
            "vocabulary size: 19936\n"
          ]
        }
      ],
      "source": [
        "# Test for the functions\n",
        "n = 2\n",
        "train, dev, test = load_data('lm')\n",
        "train_tokens = build_vocab(train, n)\n",
        "dev_tokens = build_vocab(dev, n)\n",
        "test_tokens = build_vocab(test, n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7oBATsX8uHb"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6VpAkS9odh"
      },
      "source": [
        "The vocab size of training set is 52640\n",
        "The vocab size of dev set is 19546\n",
        "The vocab size of test set is 19936\n",
        "\n",
        "The number of parameters of the model is about the size of the training token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ2BGUig8TqH"
      },
      "source": [
        "### 1.2 $n$-gram Language Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeyANMPe9ad_"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ACSfNZGE8Yw2"
      },
      "outputs": [],
      "source": [
        "def build_n_gram_model(tokens, n):\n",
        "    if n==1: # unigram model\n",
        "        freq_tokens = nltk.FreqDist(tokens)\n",
        "        len_tokens = len(tokens)\n",
        "        return {vocab: count/len_tokens for vocab, count in freq_tokens.items()}\n",
        "    else: #bigram model or trigram model\n",
        "        ngrams = nltk.ngrams(tokens, n)\n",
        "        n_minus1_grams = nltk.ngrams(tokens, n-1)\n",
        "        freq_ngrams = nltk.FreqDist(ngrams)\n",
        "        freq_n_minus1_grams = nltk.FreqDist(n_minus1_grams)\n",
        "\n",
        "        result = dict()\n",
        "        for ngram, freq in freq_ngrams.items():\n",
        "            freq_n_minus1 = freq_n_minus1_grams[ngram[:-1]]\n",
        "            result.update({ngram: freq / freq_n_minus1})\n",
        "\n",
        "        return result\n",
        "\n",
        "def calculate_perplexity(model, test_tokens, n):\n",
        "    import numpy as np\n",
        "    test_ngrams = nltk.ngrams(test_tokens, n)\n",
        "\n",
        "    result = 0\n",
        "    for ngram in list(test_ngrams):\n",
        "        if ngram in model:\n",
        "            result += np.log(model[ngram])\n",
        "    return np.exp(-1 * result / len(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_perplexity:  26174205.990303412\n",
            "test_perplexity:  6.1461403073513265\n"
          ]
        }
      ],
      "source": [
        "n_gram_model = build_n_gram_model(train_tokens, n)\n",
        "# print(n_gram_model)\n",
        "train_perplexity = calculate_perplexity(n_gram_model, train_tokens, n)\n",
        "print('train_perplexity: ', train_perplexity)\n",
        "test_perplexity = calculate_perplexity(n_gram_model, test_tokens, n)\n",
        "print('test_perplexity: ', test_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRWC56a19TbY"
      },
      "source": [
        "#### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sM4gcgL--Ylh"
      },
      "source": [
        "For the Unigram, \n",
        "\n",
        "    the perplexity on training set:  1.0\n",
        "\n",
        "    the perplexity on test test:  1.0\n",
        "\n",
        "For the Bigram model, \n",
        "\n",
        "    the perplexity on training set:  26174205.990303412\n",
        "\n",
        "    the perplexity on test test:  6.1461403073513265\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuLL8CH1Ua-3"
      },
      "source": [
        "### 1.3 Smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7mWQhaCUixZ"
      },
      "source": [
        "#### 1.3.1 Add-one (Laplace) smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbbHxLDmVrz6"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "93_yLu9dVr0C"
      },
      "outputs": [],
      "source": [
        "def build_add_one_n_gram_model(tokens, n):\n",
        "    if n==1: # unigram model\n",
        "        freq_tokens = nltk.FreqDist(tokens)\n",
        "        len_tokens = len(tokens)\n",
        "        return {vocab: count/len_tokens for vocab, count in freq_tokens.items()}\n",
        "    else: #bigram model or trigram model\n",
        "        ngrams = nltk.ngrams(tokens, n)\n",
        "        n_minus1_grams = nltk.ngrams(tokens, n-1)\n",
        "        freq_ngrams = nltk.FreqDist(ngrams)\n",
        "        freq_n_minus1_grams = nltk.FreqDist(n_minus1_grams)\n",
        "\n",
        "        result = dict()\n",
        "        for ngram, freq in freq_ngrams.items():\n",
        "            freq_n_minus1 = freq_n_minus1_grams[ngram[:-1]]\n",
        "            result.update({ngram: (freq+1) / (freq_n_minus1+1)})\n",
        "\n",
        "        return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_perplexity:  12863974.249825127\n",
            "test_perplexity:  5.880734463739251\n"
          ]
        }
      ],
      "source": [
        "n_gram_model = build_add_one_n_gram_model(train_tokens, n)\n",
        "# print(n_gram_model)\n",
        "train_perplexity = calculate_perplexity(n_gram_model, train_tokens, n)\n",
        "print('train_perplexity: ', train_perplexity)\n",
        "test_perplexity = calculate_perplexity(n_gram_model, test_tokens, n)\n",
        "print('test_perplexity: ', test_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y1WOQtsVr0D"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTknh9pRVr0D"
      },
      "source": [
        "For the training set, the perplexity of Bigram model is decreased from 26174205.990303412 to 12863974.249825127\n",
        "\n",
        "For the test set, the perplexity of Bigram model is decreased from 6.1461403073513265 to 5.880734463739251\n",
        "\n",
        "    It is shown that the perplexity of the model in both dataset decrease after implementing Add-one smoothing. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTC0qJE8VVha"
      },
      "source": [
        "##### Optional: Add-k smoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3itGMOOVuNg"
      },
      "source": [
        "###### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhcuJWo7VuNg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzpU_p6CVuNg"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIOUpNXYVuNh"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJFWxsN-Uj0Y"
      },
      "source": [
        "#### 1.3.2 Linear Interpolation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk11EpboWVCH"
      },
      "source": [
        "##### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "K4N_XuN6WVCQ"
      },
      "outputs": [],
      "source": [
        "def linear_interpolation(model1, model2, model3, lambda1, lambda2, lambda3):\n",
        "    new_model = dict()\n",
        "\n",
        "    for trigram, prob3 in model3.items():\n",
        "        prob2 = model2[trigram[1:]]\n",
        "        prob1 = model1[trigram[2]]\n",
        "        new_model.update({trigram: lambda1*prob1 + lambda2*prob2 + lambda3*prob3})\n",
        "    \n",
        "    return new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train_perplexity:  1.0\n",
            "test_perplexity:  1.0\n"
          ]
        }
      ],
      "source": [
        "# Test for the functions\n",
        "unimodel = build_n_gram_model(train_tokens, 1)\n",
        "bimodel = build_add_one_n_gram_model(train_tokens, 2)\n",
        "trimodel = build_add_one_n_gram_model(train_tokens, 3)\n",
        "\n",
        "lambda1 = 0\n",
        "lambda2 = 1/3\n",
        "lambda3 = 2/3\n",
        "\n",
        "linear_interpolation_model = linear_interpolation(unimodel, bimodel, trimodel, lambda1, lambda2, lambda3)\n",
        "train_perplexity = calculate_perplexity(linear_interpolation_model, train_tokens, n)\n",
        "print('train_perplexity: ', train_perplexity)\n",
        "test_perplexity = calculate_perplexity(linear_interpolation_model, test_tokens, n)\n",
        "print('test_perplexity: ', test_perplexity)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kh8v2P36WVCQ"
      },
      "source": [
        "##### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGkf6IV0WVCQ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvgTZcNFVato"
      },
      "source": [
        "##### Optional: Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnXu98hWcfu"
      },
      "source": [
        "###### Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKo4ZLASWcfu"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E8xYXuCUoAR"
      },
      "source": [
        "## 2 Preposition Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "gewisxM5W5kQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n",
            "'wget' ���O�����Υ~���R�O�B�i���檺�{���Χ妸�ɡC\n"
          ]
        }
      ],
      "source": [
        "!wget -O dev.in https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.in\n",
        "!wget -O dev.out https://raw.githubusercontent.com/ranpox/comp7607-fall2022/main/assignments/A1/data/prep/dev.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vocabulary size: 7312\n",
            "vocabulary size: 7310\n"
          ]
        }
      ],
      "source": [
        "train, dev, test = load_data('prep')\n",
        "\n",
        "dev_tokens = build_vocab(dev, n)\n",
        "test_tokens = build_vocab(test, n)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "eecfc76fa3750cf2629c769369211f386c60dfd205d50c8d0a6e4dde78012e31"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
